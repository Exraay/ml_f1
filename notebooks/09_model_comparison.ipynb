{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Model Comparison\n",
    "\n",
    "**Thesis: Lap Time Prediction with Physics-Based Features**\n",
    "\n",
    "This notebook performs a systematic comparison of three model architectures:\n",
    "1. **Linear Regression** (Ridge regularized) - Baseline\n",
    "2. **XGBoost** - Gradient Boosting Ensemble\n",
    "3. **Deep MLP** - Neural Network with 4 hidden layers\n",
    "\n",
    "All models use the physics-based features (Fuel Load, Tire Degradation, Track Evolution).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Project imports\n",
    "from src.data_loader import load_laps_for_seasons, clean_laps, enable_cache\n",
    "from src.features import build_feature_table\n",
    "from src.models import make_comparison_models, set_global_seed\n",
    "from src.evaluation import (\n",
    "    time_based_split,\n",
    "    compare_models,\n",
    "    create_comparison_table,\n",
    ")\n",
    "\n",
    "# Settings\n",
    "RANDOM_STATE = 42\n",
    "TEST_SEASON = 2023\n",
    "REPORTS_DIR = Path('reports')\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "set_global_seed(RANDOM_STATE)\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cache and load data\n",
    "enable_cache()\n",
    "\n",
    "# Load seasons 2022-2023\n",
    "print(\"Loading lap data for seasons 2022-2023...\")\n",
    "raw_laps = load_laps_for_seasons([2022, 2023])\n",
    "print(f\"Raw laps loaded: {len(raw_laps):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean laps\n",
    "clean_df = clean_laps(raw_laps, exclude_lap1=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature table with physics features\n",
    "feature_df, numeric_cols, categorical_cols = build_feature_table(\n",
    "    clean_df, include_physics=True, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(f\"  Numeric ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"  Categorical ({len(categorical_cols)}): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split: Train on 2022, Test on 2023\n",
    "train_df, test_df = time_based_split(feature_df, test_season=TEST_SEASON)\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"  Training: {len(train_df):,} samples (Season < {TEST_SEASON})\")\n",
    "print(f\"  Test:     {len(test_df):,} samples (Season = {TEST_SEASON})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for comparison\n",
    "feature_cols = numeric_cols + categorical_cols\n",
    "\n",
    "models = make_comparison_models(\n",
    "    numeric_features=numeric_cols,\n",
    "    categorical_features=categorical_cols,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"Models to compare:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "metrics_df, predictions, table_str = compare_models(\n",
    "    models=models,\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=\"LapTimeSeconds\",\n",
    "    baseline_model=\"Linear Regression\",\n",
    "    save_path=REPORTS_DIR / \"model_comparison_results.csv\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics table\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Visualization: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "metrics_to_plot = ['mae', 'rmse', 'r2']\n",
    "titles = ['Mean Absolute Error (s)', 'Root Mean Squared Error (s)', 'R² Score']\n",
    "colors = ['#E74C3C', '#3498DB', '#2ECC71']\n",
    "\n",
    "for ax, metric, title, color in zip(axes, metrics_to_plot, titles, colors):\n",
    "    data = metrics_df.sort_values(metric, ascending=(metric != 'r2'))\n",
    "    bars = ax.barh(data['model'], data[metric], color=color, edgecolor='black')\n",
    "    ax.set_xlabel(title)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, data[metric]):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{val:.4f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'model_comparison_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {REPORTS_DIR / 'model_comparison_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual distribution comparison\n",
    "y_test = test_df['LapTimeSeconds'].values\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, (model_name, preds) in zip(axes, predictions.items()):\n",
    "    residuals = y_test - preds\n",
    "    \n",
    "    ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax.axvline(x=np.mean(residuals), color='orange', linestyle='-', linewidth=2, label=f'Mean: {np.mean(residuals):.3f}s')\n",
    "    ax.set_xlabel('Residual (Actual - Predicted) [s]')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{model_name}\\nStd: {np.std(residuals):.3f}s')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-5, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'model_residual_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {REPORTS_DIR / 'model_residual_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatter plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, (model_name, preds) in zip(axes, predictions.items()):\n",
    "    # Sample for visibility\n",
    "    sample_idx = np.random.choice(len(y_test), size=min(2000, len(y_test)), replace=False)\n",
    "    \n",
    "    ax.scatter(y_test[sample_idx], preds[sample_idx], alpha=0.3, s=5)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    lims = [min(y_test.min(), preds.min()), max(y_test.max(), preds.max())]\n",
    "    ax.plot(lims, lims, 'r--', linewidth=2, label='Perfect prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Lap Time (s)')\n",
    "    ax.set_ylabel('Predicted Lap Time (s)')\n",
    "    ax.set_title(f'{model_name}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'model_predicted_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {REPORTS_DIR / 'model_predicted_vs_actual.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Error Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by tire compound\n",
    "best_model = metrics_df.iloc[0]['model']\n",
    "best_preds = predictions[best_model]\n",
    "\n",
    "analysis_df = test_df.copy()\n",
    "analysis_df['prediction'] = best_preds\n",
    "analysis_df['abs_error'] = np.abs(analysis_df['LapTimeSeconds'] - analysis_df['prediction'])\n",
    "\n",
    "# MAE by compound\n",
    "compound_errors = analysis_df.groupby('Compound')['abs_error'].agg(['mean', 'std', 'count'])\n",
    "compound_errors.columns = ['MAE', 'Std', 'Count']\n",
    "compound_errors = compound_errors.sort_values('MAE')\n",
    "\n",
    "print(f\"\\nError Analysis by Compound ({best_model}):\")\n",
    "print(compound_errors.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by Circuit\n",
    "circuit_errors = analysis_df.groupby('Circuit')['abs_error'].agg(['mean', 'std', 'count'])\n",
    "circuit_errors.columns = ['MAE', 'Std', 'Count']\n",
    "circuit_errors = circuit_errors.sort_values('MAE')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "circuits = circuit_errors.head(15).index.tolist()\n",
    "maes = circuit_errors.head(15)['MAE'].values\n",
    "\n",
    "bars = ax.barh(circuits, maes, color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Mean Absolute Error (s)')\n",
    "ax.set_title(f'Prediction Error by Circuit ({best_model})')\n",
    "ax.axvline(x=analysis_df['abs_error'].mean(), color='red', linestyle='--', label=f'Overall MAE: {analysis_df[\"abs_error\"].mean():.3f}s')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'error_by_circuit.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by Session Progress (track evolution effect)\n",
    "if 'SessionProgress' in analysis_df.columns:\n",
    "    analysis_df['ProgressBin'] = pd.cut(analysis_df['SessionProgress'], bins=10, labels=[f'{i*10}-{(i+1)*10}%' for i in range(10)])\n",
    "    \n",
    "    progress_errors = analysis_df.groupby('ProgressBin')['abs_error'].mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    progress_errors.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Race Progress')\n",
    "    ax.set_ylabel('Mean Absolute Error (s)')\n",
    "    ax.set_title('Prediction Error by Race Progress')\n",
    "    ax.axhline(y=analysis_df['abs_error'].mean(), color='red', linestyle='--', label='Overall MAE')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'error_by_race_progress.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Physics Features Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of physics features with prediction error\n",
    "physics_features = ['EstimatedFuelWeight', 'FuelEffect', 'TireDegradation', \n",
    "                    'EstimatedGrip', 'SessionProgress', 'TrackEvolution']\n",
    "\n",
    "available_physics = [f for f in physics_features if f in analysis_df.columns]\n",
    "\n",
    "if available_physics:\n",
    "    corr_with_error = analysis_df[available_physics + ['abs_error']].corr()['abs_error'].drop('abs_error')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    colors = ['#2ECC71' if c < 0 else '#E74C3C' for c in corr_with_error.values]\n",
    "    bars = ax.barh(corr_with_error.index, corr_with_error.values, color=colors, edgecolor='black')\n",
    "    ax.axvline(x=0, color='black', linewidth=1)\n",
    "    ax.set_xlabel('Correlation with Prediction Error')\n",
    "    ax.set_title('Physics Features Correlation with Error\\n(Green = Lower Error, Red = Higher Error)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'physics_features_error_correlation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPhysics Feature Correlation with Prediction Error:\")\n",
    "    for feat, corr in corr_with_error.items():\n",
    "        print(f\"  {feat}: {corr:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. LaTeX Table for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table\n",
    "latex_table = create_comparison_table(metrics_df, baseline_model=\"Linear Regression\", latex=True)\n",
    "print(\"LaTeX Table for Thesis:\")\n",
    "print(\"=\" * 60)\n",
    "print(latex_table)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save to file\n",
    "with open(REPORTS_DIR / 'model_comparison_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"\\nSaved: {REPORTS_DIR / 'model_comparison_table.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SCIENTIFIC MODEL COMPARISON - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Training: {len(train_df):,} laps (Season 2022)\")\n",
    "print(f\"  - Test: {len(test_df):,} laps (Season 2023)\")\n",
    "print(f\"  - Features: {len(numeric_cols)} numeric + {len(categorical_cols)} categorical\")\n",
    "\n",
    "print(f\"\\nPhysics-Based Features Used:\")\n",
    "for feat in ['EstimatedFuelWeight', 'EstimatedGrip', 'SessionProgress', 'TrackEvolution']:\n",
    "    if feat in numeric_cols:\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nModel Performance (sorted by MAE):\")\n",
    "for _, row in metrics_df.iterrows():\n",
    "    print(f\"  {row['model']:<20} MAE: {row['mae']:.4f}s  R²: {row['r2']:.4f}\")\n",
    "\n",
    "best = metrics_df.iloc[0]\n",
    "baseline = metrics_df[metrics_df['model'] == 'Linear Regression'].iloc[0]\n",
    "improvement = (baseline['mae'] - best['mae']) / baseline['mae'] * 100\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  - Best Model: {best['model']}\")\n",
    "print(f\"  - MAE Improvement over Baseline: {improvement:.1f}%\")\n",
    "print(f\"  - Best MAE: {best['mae']:.4f} seconds\")\n",
    "print(f\"  - Best R²: {best['r2']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved files\n",
    "print(\"\\nGenerated Reports:\")\n",
    "for f in sorted(REPORTS_DIR.glob('*')):\n",
    "    print(f\"  - {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
