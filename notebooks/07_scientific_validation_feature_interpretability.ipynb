{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scientific Validation & Feature Interpretability\n",
        "\n",
        "This notebook is a defense document for the seminar paper. It addresses common examiner questions about data integrity, leakage, feature validity, model choice, and error behavior.\n",
        "\n",
        "**Citations:** FastF1 documentation (FastF1, 2024) and Theisen (2021) are referenced in relevant sections as required by the FOM guidelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ensure repo root is on sys.path so `src` is importable\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd().resolve().parent if Path.cwd().name == 'notebooks' else Path.cwd().resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "print(f'Using project root: {ROOT}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    _SEABORN_OK = True\n",
        "except Exception:\n",
        "    _SEABORN_OK = False\n",
        "    print(\"seaborn not installed. Run: pip install seaborn\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from src.data import load_laps_for_seasons, clean_laps\n",
        "from _common import load_dataset, prepare_features, ROOT\n",
        "from src.split import SplitConfig\n",
        "from src.eval import compute_metrics\n",
        "from src.models import make_model_registry, build_search\n",
        "import joblib\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Integrity (Defense: *Is your data clean?*)\n",
        "\n",
        "**Visual Statistics:** Before/After cleaning summary and exact counts for the key removal rules.\n",
        "**Outlier Justification:** Lap time distribution before and after cleaning to show extreme SC/incident laps removed.\n",
        "\n",
        "Citations: FastF1 documentation (FastF1, 2024).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "YEARS = [2022, 2023, 2024, 2025]\n",
        "EXCLUDE_LAP1 = False\n",
        "\n",
        "# Load raw laps (uses FastF1 cache when available)\n",
        "raw = load_laps_for_seasons(YEARS)\n",
        "raw = raw.copy()\n",
        "raw[\"LapTimeSeconds\"] = raw[\"LapTime\"].dt.total_seconds()\n",
        "\n",
        "# Use clean_laps to execute the official cleaning rules and print stats\n",
        "clean = clean_laps(raw, exclude_lap1=EXCLUDE_LAP1, verbose=True)\n",
        "clean = clean.copy()\n",
        "clean[\"LapTimeSeconds\"] = clean[\"LapTime\"].dt.total_seconds()\n",
        "\n",
        "# Recompute filter counts following the same order as clean_laps for key reasons\n",
        "filter_counts = {}\n",
        "\n",
        "def _apply_filter(df, mask, name, record=False):\n",
        "    removed = int((~mask).sum())\n",
        "    if record:\n",
        "        filter_counts[name] = removed\n",
        "    return df[mask]\n",
        "\n",
        "_tmp = raw.copy()\n",
        "if \"LapTime\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, _tmp[\"LapTime\"].notna(), \"No LapTime\")\n",
        "if \"IsAccurate\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, _tmp[\"IsAccurate\"] == True, \"IsAccurate=False\", True)  # noqa: E712\n",
        "if \"PitOutTime\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, ~_tmp[\"PitOutTime\"].notna(), \"Pit-Out\")\n",
        "if \"PitInTime\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, ~_tmp[\"PitInTime\"].notna(), \"Pit-In\", True)\n",
        "if \"TrackStatus\" in _tmp.columns:\n",
        "    safety_mask = _tmp[\"TrackStatus\"].astype(str).str.contains(\"[4567]\", na=False)\n",
        "    _tmp = _apply_filter(_tmp, ~safety_mask, \"SC/VSC/RedFlag\", True)\n",
        "if \"Deleted\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, _tmp[\"Deleted\"] != True, \"Deleted\")  # noqa: E712\n",
        "if \"LapNumber\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, _tmp[\"LapNumber\"] > 0, \"Formation Lap\")\n",
        "if EXCLUDE_LAP1 and \"LapNumber\" in _tmp.columns:\n",
        "    _tmp = _apply_filter(_tmp, _tmp[\"LapNumber\"] > 1, \"Lap 1 (Standing Start)\")\n",
        "\n",
        "# Summary table\n",
        "summary = pd.DataFrame({\n",
        "    \"Stage\": [\"Before cleaning\", \"After cleaning\"],\n",
        "    \"Lap count\": [len(raw), len(clean)],\n",
        "})\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Key removal counts (exact reasons requested)\n",
        "key_counts = pd.DataFrame({\n",
        "    \"Reason\": [\"IsAccurate=False\", \"Pit-In\", \"SC/VSC/RedFlag\"],\n",
        "    \"Laps removed\": [\n",
        "        filter_counts.get(\"IsAccurate=False\", 0),\n",
        "        filter_counts.get(\"Pit-In\", 0),\n",
        "        filter_counts.get(\"SC/VSC/RedFlag\", 0),\n",
        "    ],\n",
        "})\n",
        "key_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visual summary: before vs after cleaning\n",
        "fig = px.bar(\n",
        "    summary,\n",
        "    x=\"Stage\",\n",
        "    y=\"Lap count\",\n",
        "    color=\"Stage\",\n",
        "    title=\"Before vs After Cleaning: Lap Count\",\n",
        "    labels={\"Stage\": \"Dataset stage\", \"Lap count\": \"Number of laps\"},\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Outlier justification: histogram before vs after cleaning\n",
        "fig = go.Figure()\n",
        "fig.add_histogram(\n",
        "    x=raw[\"LapTimeSeconds\"].dropna(),\n",
        "    name=\"Before\",\n",
        "    opacity=0.6,\n",
        "    nbinsx=80,\n",
        ")\n",
        "fig.add_histogram(\n",
        "    x=clean[\"LapTimeSeconds\"].dropna(),\n",
        "    name=\"After\",\n",
        "    opacity=0.6,\n",
        "    nbinsx=80,\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"LapTimeSeconds Distribution Before vs After Cleaning\",\n",
        "    xaxis_title=\"Lap time (seconds)\",\n",
        "    yaxis_title=\"Count\",\n",
        "    barmode=\"overlay\",\n",
        "    legend_title=\"Dataset\",\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Temporal Splitting (Defense: *Did you cheat with Data Leakage?*)\n",
        "\n",
        "**Methodological note:** Time-series splitting is mandatory because F1 is sequential. A random shuffle would allow the model to \"see\" future car setups and track conditions when predicting earlier laps, which is scientifically invalid.\n",
        "\n",
        "Citations: FastF1 documentation (FastF1, 2024); Theisen (2021).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Timeline plot: Train (2022-2023) vs Test (2024-2025)\n",
        "if \"SessionDate\" not in clean.columns:\n",
        "    raise KeyError(\"SessionDate not found in laps; expected from FastF1 session load.\")\n",
        "\n",
        "timeline = clean[[\"SessionDate\", \"LapTimeSeconds\", \"Season\"]].dropna().copy()\n",
        "timeline[\"Split\"] = np.where(timeline[\"Season\"] <= 2023, \"Train (2022-2023)\", \"Test (2024-2025)\")\n",
        "\n",
        "# Sample for visualization clarity\n",
        "sample_n = min(50000, len(timeline))\n",
        "plot_df = timeline.sample(sample_n, random_state=42)\n",
        "\n",
        "fig = px.scatter(\n",
        "    plot_df,\n",
        "    x=\"SessionDate\",\n",
        "    y=\"LapTimeSeconds\",\n",
        "    color=\"Split\",\n",
        "    title=\"Temporal Split: Training vs Test Data\",\n",
        "    labels={\"SessionDate\": \"Date\", \"LapTimeSeconds\": \"Lap time (seconds)\"},\n",
        ")\n",
        "fig.update_layout(legend_title=\"Split\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Physics Feature Impact (Defense: *Do engineered features actually matter?*)\n",
        "\n",
        "This section validates that physics-inspired features have measurable statistical relationships with lap time.\n",
        "\n",
        "Citations: FastF1 documentation (FastF1, 2024); Theisen (2021).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load processed feature dataset\n",
        "feature_df, metadata = load_dataset()\n",
        "\n",
        "required_cols = [\"LapTimeSeconds\", \"EstimatedFuelWeight\", \"EstimatedGrip\", \"TrackEvolution\", \"TyreLife\", \"Compound\"]\n",
        "missing = [c for c in required_cols if c not in feature_df.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns in processed dataset: {missing}\")\n",
        "\n",
        "# Correlation heatmap (seaborn)\n",
        "if _SEABORN_OK:\n",
        "    corr_cols = [\"LapTimeSeconds\", \"EstimatedFuelWeight\", \"EstimatedGrip\", \"TrackEvolution\"]\n",
        "    corr = feature_df[corr_cols].corr(numeric_only=True)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar_kws={\"label\": \"Correlation\"})\n",
        "    plt.title(\"Correlation Heatmap: LapTimeSeconds vs Physics Features\")\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Correlation ranking (absolute correlation with LapTimeSeconds)\n",
        "features = [\"EstimatedFuelWeight\", \"EstimatedGrip\", \"TrackEvolution\"]\n",
        "\n",
        "corr_series = (\n",
        "    feature_df[[\"LapTimeSeconds\"] + features]\n",
        "    .corr(numeric_only=True)[\"LapTimeSeconds\"]\n",
        "    .drop(\"LapTimeSeconds\")\n",
        "    .abs()\n",
        "    .sort_values()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.barh(corr_series.index, corr_series.values)\n",
        "plt.title(\"Absolute Correlation with LapTimeSeconds\")\n",
        "plt.xlabel(\"|Correlation|\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.legend([\"|Correlation|\"])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Physics sanity check: EstimatedGrip vs TyreLife by compound\n",
        "plot_df = feature_df.copy()\n",
        "plot_df[\"Compound\"] = plot_df[\"Compound\"].astype(str).str.upper()\n",
        "plot_df = plot_df[plot_df[\"Compound\"].isin([\"SOFT\", \"MEDIUM\", \"HARD\"])].dropna(subset=[\"TyreLife\", \"EstimatedGrip\"])\n",
        "\n",
        "# Sample for clarity\n",
        "plot_df = plot_df.sample(min(30000, len(plot_df)), random_state=42)\n",
        "\n",
        "if _SEABORN_OK:\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    sns.lineplot(\n",
        "        data=plot_df,\n",
        "        x=\"TyreLife\",\n",
        "        y=\"EstimatedGrip\",\n",
        "        hue=\"Compound\",\n",
        "        estimator=\"median\",\n",
        "        errorbar=None,\n",
        "    )\n",
        "    plt.title(\"EstimatedGrip vs TyreLife by Compound\")\n",
        "    plt.xlabel(\"TyreLife (laps)\")\n",
        "    plt.ylabel(\"EstimatedGrip\")\n",
        "    plt.legend(title=\"Compound\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison (Defense: *Why this model?*)\n",
        "\n",
        "This section compares MAE, RMSE, and $R^2$ across Linear, XGBoost, and Deep MLP. It also visualizes MAE vs RMSE to discuss bias vs variance.\n",
        "\n",
        "Note: By default, this notebook **loads saved models** from `reports/models/` to avoid retraining and hyperparameter confusion. Set `LOAD_MODELS_IF_AVAILABLE = False` to retrain.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "TUNE_MODE = \"off\"  # used only if training is required\n",
        "LOAD_MODELS_IF_AVAILABLE = True\n",
        "\n",
        "MODELS_DIR = ROOT / \"reports\" / \"models\"\n",
        "MODEL_PATHS = {\n",
        "    \"Linear\": MODELS_DIR / \"linear.joblib\",\n",
        "    \"XGBoost\": MODELS_DIR / \"xgboost.joblib\",\n",
        "    \"Deep MLP\": MODELS_DIR / \"deep_mlp.joblib\",\n",
        "}\n",
        "\n",
        "split_config = SplitConfig(test_rounds=6)\n",
        "df, metadata = load_dataset()\n",
        "train_df, val_df, trainval_df, test_df, features = prepare_features(df, metadata, split_config=split_config)\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[\"LapTimeSeconds\"].to_numpy()\n",
        "X_val = val_df[features]\n",
        "y_val = val_df[\"LapTimeSeconds\"].to_numpy()\n",
        "X_trainval = trainval_df[features]\n",
        "y_trainval = trainval_df[\"LapTimeSeconds\"].to_numpy()\n",
        "X_test = test_df[features]\n",
        "y_test = test_df[\"LapTimeSeconds\"].to_numpy()\n",
        "\n",
        "fitted = {}\n",
        "if LOAD_MODELS_IF_AVAILABLE and all(p.exists() for p in MODEL_PATHS.values()):\n",
        "    for name, path in MODEL_PATHS.items():\n",
        "        fitted[name] = joblib.load(path)\n",
        "else:\n",
        "    base_models = make_model_registry(features, random_state=SEED)\n",
        "    models = {k: build_search(k, v, random_state=SEED, mode=TUNE_MODE) for k, v in base_models.items()}\n",
        "    from src.eval import evaluate_models\n",
        "    _, _, tmp = evaluate_models(models, X_train, y_train, X_val, y_val)\n",
        "    for name, estimator in tmp.items():\n",
        "        best = estimator.best_estimator_ if hasattr(estimator, \"best_estimator_\") else estimator\n",
        "        best.fit(X_trainval, y_trainval)\n",
        "        fitted[name] = best\n",
        "\n",
        "# Compute test metrics\n",
        "rows = []\n",
        "for name, model in fitted.items():\n",
        "    preds = model.predict(X_test)\n",
        "    scores = compute_metrics(y_test, preds)\n",
        "    scores[\"model\"] = name\n",
        "    rows.append(scores)\n",
        "\n",
        "metrics_test = pd.DataFrame(rows).sort_values(\"mae\").reset_index(drop=True)\n",
        "metrics_test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Bias-variance discussion: MAE vs RMSE\n",
        "fig = px.scatter(\n",
        "    metrics_test,\n",
        "    x=\"mae\",\n",
        "    y=\"rmse\",\n",
        "    color=\"model\",\n",
        "    text=\"model\",\n",
        "    title=\"MAE vs RMSE (Bias-Variance Indicator)\",\n",
        "    labels={\"mae\": \"MAE (s)\", \"rmse\": \"RMSE (s)\"},\n",
        ")\n",
        "fig.update_traces(textposition=\"top center\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Error Analysis (Defense: *Where does your model fail?*)\n",
        "\n",
        "We analyze residuals and error by Circuit and Compound using the best test model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Pick best model by MAE\n",
        "best_row = metrics_test.iloc[0]\n",
        "best_name = best_row[\"model\"]\n",
        "best_model = fitted[best_name]\n",
        "\n",
        "preds = best_model.predict(X_test)\n",
        "residuals = y_test - preds\n",
        "\n",
        "# Residual distribution\n",
        "fig = px.histogram(\n",
        "    residuals,\n",
        "    nbins=60,\n",
        "    title=f\"Residual Distribution (Actual - Predicted): {best_name}\",\n",
        "    labels={\"value\": \"Residual (seconds)\", \"count\": \"Count\"},\n",
        ")\n",
        "fig.update_traces(name=\"Residuals\", showlegend=True)\n",
        "fig.update_layout(showlegend=True, legend_title=\"Series\")\n",
        "fig.show()\n",
        "\n",
        "# Error by category: Circuit and Compound\n",
        "errors_df = test_df.copy()\n",
        "errors_df[\"Pred\"] = preds\n",
        "errors_df[\"AbsError\"] = np.abs(errors_df[\"Pred\"] - errors_df[\"LapTimeSeconds\"])\n",
        "\n",
        "if \"Circuit\" in errors_df.columns:\n",
        "    circuit_mae = (\n",
        "        errors_df.groupby(\"Circuit\")\n",
        "        .agg(mae=(\"AbsError\", \"mean\"))\n",
        "        .sort_values(\"mae\", ascending=False)\n",
        "        .head(15)\n",
        "        .reset_index()\n",
        "    )\n",
        "    fig = px.bar(\n",
        "        circuit_mae,\n",
        "        x=\"mae\",\n",
        "        y=\"Circuit\",\n",
        "        color=\"mae\",\n",
        "        orientation=\"h\",\n",
        "        title=f\"MAE by Circuit (Top 15) - {best_name}\",\n",
        "        labels={\"mae\": \"MAE (s)\", \"Circuit\": \"Circuit\"},\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "if \"Compound\" in errors_df.columns:\n",
        "    comp_mae = (\n",
        "        errors_df.groupby(\"Compound\")\n",
        "        .agg(mae=(\"AbsError\", \"mean\"))\n",
        "        .sort_values(\"mae\", ascending=False)\n",
        "        .reset_index()\n",
        "    )\n",
        "    fig = px.bar(\n",
        "        comp_mae,\n",
        "        x=\"Compound\",\n",
        "        y=\"mae\",\n",
        "        title=f\"MAE by Compound - {best_name}\",\n",
        "        labels={\"mae\": \"MAE (s)\", \"Compound\": \"Compound\"},\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Future Outlook (A+ Bonus)\n",
        "\n",
        "**Attention / Transformer concept:** The current model uses sliding-window lags (e.g., LapTimeLag1?3) which only capture short-term history. A Transformer with attention could ingest the **entire race sequence** per driver and learn long-range dependencies such as strategy phases, traffic effects, and evolving track conditions. This would allow the model to weigh distant but relevant events (e.g., pit stops, safety car periods) when predicting current lap performance.\n",
        "\n",
        "Citations: Theisen (2021); FastF1 documentation (FastF1, 2024).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}