{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 - Model Comparison (Full 2025 Test)\n",
        "\n",
        "This notebook loads the saved models (`.joblib`) and evaluates them on **all available 2025 race data** in the processed dataset.\n",
        "It produces publication-ready plots and a comprehensive set of numerical metrics for the paper.\n",
        "\n",
        "Outputs are saved to `reports/notebooks/06_model_comparison_full/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd().resolve().parent if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "print(f\"Using project root: {ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import joblib\n",
        "\n",
        "from _common import load_dataset, prepare_features\n",
        "from src.split import SplitConfig\n",
        "from src.plots import plot_actual_vs_pred, plot_error_distribution, save_plot\n",
        "\n",
        "OUTPUT_DIR = ROOT / \"reports\" / \"notebooks\" / \"06_model_comparison_full\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load dataset and build the 2025 test split\n",
        "split_config = SplitConfig(test_rounds=None)\n",
        "df, metadata = load_dataset()\n",
        "train_df, val_df, trainval_df, test_df, features = prepare_features(df, metadata, split_config=split_config)\n",
        "\n",
        "X_test = test_df[features]\n",
        "y_test = test_df[\"LapTimeSeconds\"].to_numpy()\n",
        "\n",
        "summary = (\n",
        "    test_df[[\"Season\", \"RoundNumber\", \"EventName\"]]\n",
        "    .drop_duplicates()\n",
        "    .sort_values([\"Season\", \"RoundNumber\"])\n",
        ")\n",
        "print(f\"Test rows: {len(test_df):,}\")\n",
        "print(f\"Test seasons: {sorted(test_df['Season'].unique())}\")\n",
        "print(f\"Test rounds: {summary['RoundNumber'].tolist()}\")\n",
        "summary.head(12)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load saved models\n",
        "MODELS_DIR = ROOT / \"reports\" / \"models\"\n",
        "MODEL_PATHS = {\n",
        "    \"Linear\": MODELS_DIR / \"linear.joblib\",\n",
        "    \"XGBoost\": MODELS_DIR / \"xgboost.joblib\",\n",
        "    \"Deep MLP\": MODELS_DIR / \"deep_mlp.joblib\",\n",
        "}\n",
        "\n",
        "missing = [name for name, path in MODEL_PATHS.items() if not path.exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(f\"Missing saved models: {missing}. Expected in {MODELS_DIR}\")\n",
        "\n",
        "models = {name: joblib.load(path) for name, path in MODEL_PATHS.items()}\n",
        "models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "preds = {name: model.predict(X_test) for name, model in models.items()}\n",
        "\n",
        "# Shared error frame (for group-level analysis and plots)\n",
        "meta_cols = [\n",
        "    c for c in [\n",
        "        \"Season\", \"RoundNumber\", \"EventName\", \"Driver\", \"Team\", \"Compound\",\n",
        "        \"Circuit\", \"TireAgeCategory\", \"LapNumber\", \"Stint\", \"TyreLife\"\n",
        "    ] if c in test_df.columns\n",
        "]\n",
        "\n",
        "frames = []\n",
        "for name, y_pred in preds.items():\n",
        "    temp = test_df[meta_cols].copy()\n",
        "    temp[\"model\"] = name\n",
        "    temp[\"y_true\"] = y_test\n",
        "    temp[\"y_pred\"] = y_pred\n",
        "    temp[\"error\"] = y_pred - y_test\n",
        "    temp[\"abs_error\"] = np.abs(temp[\"error\"])\n",
        "    frames.append(temp)\n",
        "\n",
        "errors_df = pd.concat(frames, ignore_index=True)\n",
        "errors_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    max_error,\n",
        "    explained_variance_score,\n",
        "    mean_squared_log_error,\n",
        ")\n",
        "\n",
        "def style_plot(fig, *, height=480, width=None, title=None):\n",
        "    fig.update_layout(\n",
        "        template=\"plotly_white\",\n",
        "        height=height,\n",
        "        width=width,\n",
        "        title=title or fig.layout.title.text,\n",
        "        font=dict(size=14),\n",
        "    )\n",
        "    fig.update_xaxes(\n",
        "        showline=True,\n",
        "        linewidth=1,\n",
        "        linecolor=\"black\",\n",
        "        mirror=True,\n",
        "        ticks=\"outside\",\n",
        "        showgrid=True,\n",
        "        gridcolor=\"rgba(0,0,0,0.1)\",\n",
        "    )\n",
        "    fig.update_yaxes(\n",
        "        showline=True,\n",
        "        linewidth=1,\n",
        "        linecolor=\"black\",\n",
        "        mirror=True,\n",
        "        ticks=\"outside\",\n",
        "        showgrid=True,\n",
        "        gridcolor=\"rgba(0,0,0,0.1)\",\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def compute_full_metrics(y_true, y_pred, n_features):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    resid = y_pred - y_true\n",
        "    abs_err = np.abs(resid)\n",
        "    n = len(y_true)\n",
        "    eps = 1e-6\n",
        "\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = float(np.sqrt(mse))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    adj_r2 = (\n",
        "        1 - (1 - r2) * (n - 1) / max(n - n_features - 1, 1)\n",
        "        if n > n_features + 1 else np.nan\n",
        "    )\n",
        "\n",
        "    mape = float(np.mean(abs_err / np.clip(np.abs(y_true), eps, None)) * 100)\n",
        "    smape = float(np.mean(2 * abs_err / (np.abs(y_true) + np.abs(y_pred) + eps)) * 100)\n",
        "    rmspe = float(np.sqrt(np.mean((resid / np.clip(y_true, eps, None)) ** 2)) * 100)\n",
        "\n",
        "    try:\n",
        "        msle = mean_squared_log_error(\n",
        "            np.clip(y_true, eps, None),\n",
        "            np.clip(y_pred, eps, None),\n",
        "        )\n",
        "        rmsle = float(np.sqrt(msle))\n",
        "    except Exception:\n",
        "        msle = np.nan\n",
        "        rmsle = np.nan\n",
        "\n",
        "    bias = float(resid.mean())\n",
        "    std_err = float(resid.std())\n",
        "    med_err = float(np.median(resid))\n",
        "    mad = float(np.median(np.abs(resid - np.median(resid))))\n",
        "\n",
        "    pearson = float(np.corrcoef(y_true, y_pred)[0, 1]) if n > 1 else np.nan\n",
        "    spearman = float(pd.Series(y_true).corr(pd.Series(y_pred), method=\"spearman\")) if n > 1 else np.nan\n",
        "\n",
        "    return {\n",
        "        \"n\": n,\n",
        "        \"mae\": mae,\n",
        "        \"mse\": mse,\n",
        "        \"rmse\": rmse,\n",
        "        \"r2\": r2,\n",
        "        \"adj_r2\": adj_r2,\n",
        "        \"explained_variance\": explained_variance_score(y_true, y_pred),\n",
        "        \"mape_pct\": mape,\n",
        "        \"smape_pct\": smape,\n",
        "        \"rmspe_pct\": rmspe,\n",
        "        \"medae\": median_absolute_error(y_true, y_pred),\n",
        "        \"max_error\": max_error(y_true, y_pred),\n",
        "        \"msle\": msle,\n",
        "        \"rmsle\": rmsle,\n",
        "        \"bias_mean_error\": bias,\n",
        "        \"median_error\": med_err,\n",
        "        \"std_error\": std_err,\n",
        "        \"mad_error\": mad,\n",
        "        \"pearson_r\": pearson,\n",
        "        \"spearman_r\": spearman,\n",
        "        \"p50_abs_error\": float(np.percentile(abs_err, 50)),\n",
        "        \"p90_abs_error\": float(np.percentile(abs_err, 90)),\n",
        "        \"p95_abs_error\": float(np.percentile(abs_err, 95)),\n",
        "        \"p99_abs_error\": float(np.percentile(abs_err, 99)),\n",
        "        \"within_1s_pct\": float(np.mean(abs_err <= 1.0) * 100),\n",
        "        \"within_2s_pct\": float(np.mean(abs_err <= 2.0) * 100),\n",
        "        \"within_5s_pct\": float(np.mean(abs_err <= 5.0) * 100),\n",
        "        \"mean_true\": float(np.mean(y_true)),\n",
        "        \"std_true\": float(np.std(y_true)),\n",
        "        \"mean_pred\": float(np.mean(y_pred)),\n",
        "        \"std_pred\": float(np.std(y_pred)),\n",
        "    }\n",
        "\n",
        "def plot_residuals_vs_pred(y_true, y_pred, title):\n",
        "    df_plot = pd.DataFrame({\n",
        "        \"Predicted\": y_pred,\n",
        "        \"Residual\": y_pred - y_true,\n",
        "    })\n",
        "    fig = px.scatter(\n",
        "        df_plot,\n",
        "        x=\"Predicted\",\n",
        "        y=\"Residual\",\n",
        "        title=title,\n",
        "        opacity=0.5,\n",
        "    )\n",
        "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
        "    fig.update_xaxes(title=\"Predicted lap time (s)\")\n",
        "    fig.update_yaxes(title=\"Residual (s)\")\n",
        "    return fig\n",
        "\n",
        "def plot_calibration(y_true, y_pred, title, bins=20):\n",
        "    df_plot = pd.DataFrame({\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred,\n",
        "    })\n",
        "    df_plot[\"bin\"] = pd.qcut(df_plot[\"y_pred\"], q=bins, duplicates=\"drop\")\n",
        "    grouped = df_plot.groupby(\"bin\", observed=True).agg(\n",
        "        pred_mean=(\"y_pred\", \"mean\"),\n",
        "        true_mean=(\"y_true\", \"mean\"),\n",
        "        n=(\"y_true\", \"size\"),\n",
        "    ).reset_index()\n",
        "    fig = go.Figure()\n",
        "    fig.add_scatter(\n",
        "        x=grouped[\"pred_mean\"],\n",
        "        y=grouped[\"true_mean\"],\n",
        "        mode=\"lines+markers\",\n",
        "        name=\"Binned mean\",\n",
        "    )\n",
        "    min_v = float(min(grouped[\"pred_mean\"].min(), grouped[\"true_mean\"].min()))\n",
        "    max_v = float(max(grouped[\"pred_mean\"].max(), grouped[\"true_mean\"].max()))\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[min_v, max_v],\n",
        "            y=[min_v, max_v],\n",
        "            mode=\"lines\",\n",
        "            line=dict(color=\"red\", dash=\"dash\"),\n",
        "            name=\"Perfect\",\n",
        "        )\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Predicted (binned mean, s)\",\n",
        "        yaxis_title=\"Actual (binned mean, s)\",\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def plot_error_by_category_custom(df, category_col, title, top_n=12):\n",
        "    grouped = (\n",
        "        df.groupby(category_col, dropna=False)[\"abs_error\"]\n",
        "        .mean()\n",
        "        .sort_values(ascending=False)\n",
        "        .head(top_n)\n",
        "        .reset_index()\n",
        "    )\n",
        "    fig = px.bar(\n",
        "        grouped,\n",
        "        x=\"abs_error\",\n",
        "        y=category_col,\n",
        "        orientation=\"h\",\n",
        "        title=title,\n",
        "        labels={\"abs_error\": \"MAE (s)\", category_col: category_col},\n",
        "    )\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Overall metrics table\n",
        "rows = []\n",
        "for name, y_pred in preds.items():\n",
        "    scores = compute_full_metrics(y_test, y_pred, n_features=len(features))\n",
        "    scores[\"model\"] = name\n",
        "    rows.append(scores)\n",
        "\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "metrics_df = metrics_df.sort_values(\"mae\").reset_index(drop=True)\n",
        "\n",
        "# Reorder for readability\n",
        "cols = [\n",
        "    \"model\", \"n\",\n",
        "    \"mae\", \"rmse\", \"r2\", \"adj_r2\", \"explained_variance\",\n",
        "    \"mape_pct\", \"smape_pct\", \"rmspe_pct\",\n",
        "    \"medae\", \"max_error\",\n",
        "    \"bias_mean_error\", \"median_error\", \"std_error\", \"mad_error\",\n",
        "    \"pearson_r\", \"spearman_r\",\n",
        "    \"p50_abs_error\", \"p90_abs_error\", \"p95_abs_error\", \"p99_abs_error\",\n",
        "    \"within_1s_pct\", \"within_2s_pct\", \"within_5s_pct\",\n",
        "    \"mse\", \"msle\", \"rmsle\",\n",
        "    \"mean_true\", \"std_true\", \"mean_pred\", \"std_pred\",\n",
        "]\n",
        "metrics_df = metrics_df[cols]\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Save metrics to CSV for the paper\n",
        "metrics_df.to_csv(OUTPUT_DIR / \"metrics_overall_2025.csv\", index=False)\n",
        "\n",
        "# Also save per-lap predictions for reproducibility\n",
        "errors_df.to_parquet(OUTPUT_DIR / \"predictions_2025.parquet\", index=False)\n",
        "errors_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Bootstrap confidence intervals for key metrics\n",
        "def bootstrap_ci(y_true, y_pred, n_boot=200, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    stats = {\"mae\": [], \"rmse\": [], \"r2\": [], \"mape_pct\": []}\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt = y_true[idx]\n",
        "        yp = y_pred[idx]\n",
        "        stats[\"mae\"].append(mean_absolute_error(yt, yp))\n",
        "        stats[\"rmse\"].append(np.sqrt(mean_squared_error(yt, yp)))\n",
        "        stats[\"r2\"].append(r2_score(yt, yp))\n",
        "        stats[\"mape_pct\"].append(float(np.mean(np.abs(yp - yt) / np.clip(np.abs(yt), 1e-6, None)) * 100))\n",
        "    rows = []\n",
        "    for metric, values in stats.items():\n",
        "        low, high = np.percentile(values, [2.5, 97.5])\n",
        "        rows.append({\n",
        "            \"metric\": metric,\n",
        "            \"ci_low\": float(low),\n",
        "            \"ci_high\": float(high),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "ci_frames = []\n",
        "for name, y_pred in preds.items():\n",
        "    ci = bootstrap_ci(y_test, y_pred, n_boot=200, seed=42)\n",
        "    ci[\"model\"] = name\n",
        "    ci_frames.append(ci)\n",
        "\n",
        "ci_df = pd.concat(ci_frames, ignore_index=True)\n",
        "ci_df.to_csv(OUTPUT_DIR / \"metrics_bootstrap_ci_2025.csv\", index=False)\n",
        "ci_df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Model comparison plots\n",
        "mae_rmse = metrics_df[[\"model\", \"mae\", \"rmse\"]].melt(\n",
        "    id_vars=\"model\", var_name=\"metric\", value_name=\"value\"\n",
        ")\n",
        "fig = px.bar(\n",
        "    mae_rmse,\n",
        "    x=\"model\",\n",
        "    y=\"value\",\n",
        "    color=\"metric\",\n",
        "    barmode=\"group\",\n",
        "    title=\"Model Comparison: MAE vs RMSE (2025)\",\n",
        "    labels={\"value\": \"Seconds\", \"model\": \"Model\"},\n",
        ")\n",
        "style_plot(fig, height=480)\n",
        "save_plot(fig, OUTPUT_DIR / \"model_comparison_mae_rmse\")\n",
        "fig.show()\n",
        "\n",
        "mape_smape = metrics_df[[\"model\", \"mape_pct\", \"smape_pct\"]].melt(\n",
        "    id_vars=\"model\", var_name=\"metric\", value_name=\"value\"\n",
        ")\n",
        "fig = px.bar(\n",
        "    mape_smape,\n",
        "    x=\"model\",\n",
        "    y=\"value\",\n",
        "    color=\"metric\",\n",
        "    barmode=\"group\",\n",
        "    title=\"Model Comparison: MAPE vs sMAPE (2025)\",\n",
        "    labels={\"value\": \"Percent\", \"model\": \"Model\"},\n",
        ")\n",
        "style_plot(fig, height=480)\n",
        "save_plot(fig, OUTPUT_DIR / \"model_comparison_mape_smape\")\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(\n",
        "    metrics_df,\n",
        "    x=\"model\",\n",
        "    y=\"r2\",\n",
        "    color=\"model\",\n",
        "    title=\"Model Comparison: R2 (2025)\",\n",
        "    labels={\"r2\": \"R2\", \"model\": \"Model\"},\n",
        ")\n",
        "style_plot(fig, height=450)\n",
        "save_plot(fig, OUTPUT_DIR / \"model_comparison_r2\")\n",
        "fig.show()\n",
        "\n",
        "fig = px.scatter(\n",
        "    metrics_df,\n",
        "    x=\"mae\",\n",
        "    y=\"rmse\",\n",
        "    color=\"model\",\n",
        "    text=\"model\",\n",
        "    title=\"MAE vs RMSE (Bias-Variance Indicator)\",\n",
        "    labels={\"mae\": \"MAE (s)\", \"rmse\": \"RMSE (s)\"},\n",
        ")\n",
        "fig.update_traces(textposition=\"top center\")\n",
        "style_plot(fig, height=480)\n",
        "save_plot(fig, OUTPUT_DIR / \"mae_vs_rmse\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Detailed diagnostics per model\n",
        "for name, y_pred in preds.items():\n",
        "    # Actual vs Predicted\n",
        "    fig = plot_actual_vs_pred(y_test, y_pred, title=f\"Actual vs Predicted - {name} (2025)\", sample_size=6000)\n",
        "    fig.update_xaxes(title=\"Actual lap time (s)\")\n",
        "    fig.update_yaxes(title=\"Predicted lap time (s)\")\n",
        "    style_plot(fig, height=520)\n",
        "    save_plot(fig, OUTPUT_DIR / f\"actual_vs_pred_{name.lower().replace(' ', '_')}\")\n",
        "    fig.show()\n",
        "\n",
        "    # Residual distribution\n",
        "    fig = plot_error_distribution(y_test, y_pred, title=f\"Residual Distribution - {name} (2025)\", bins=70)\n",
        "    fig.update_xaxes(title=\"Residual (s)\")\n",
        "    fig.update_yaxes(title=\"Count\")\n",
        "    style_plot(fig, height=420)\n",
        "    save_plot(fig, OUTPUT_DIR / f\"residuals_{name.lower().replace(' ', '_')}\")\n",
        "    fig.show()\n",
        "\n",
        "    # Residuals vs Predicted\n",
        "    fig = plot_residuals_vs_pred(y_test, y_pred, title=f\"Residuals vs Predicted - {name} (2025)\")\n",
        "    style_plot(fig, height=480)\n",
        "    save_plot(fig, OUTPUT_DIR / f\"residuals_vs_pred_{name.lower().replace(' ', '_')}\")\n",
        "    fig.show()\n",
        "\n",
        "    # Calibration plot\n",
        "    fig = plot_calibration(y_test, y_pred, title=f\"Calibration (Binned Means) - {name} (2025)\", bins=20)\n",
        "    style_plot(fig, height=480)\n",
        "    save_plot(fig, OUTPUT_DIR / f\"calibration_{name.lower().replace(' ', '_')}\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Error by category (top 12 worst) for each model\n",
        "category_cols = [\"EventName\", \"Circuit\", \"Driver\", \"Team\", \"Compound\", \"TireAgeCategory\"]\n",
        "category_cols = [c for c in category_cols if c in errors_df.columns]\n",
        "\n",
        "for name in models.keys():\n",
        "    subset = errors_df[errors_df[\"model\"] == name]\n",
        "    for col in category_cols:\n",
        "        # Save numeric group metrics\n",
        "        group_df = (\n",
        "            subset.groupby(col, dropna=False)\n",
        "            .agg(\n",
        "                n=(\"abs_error\", \"size\"),\n",
        "                mae=(\"abs_error\", \"mean\"),\n",
        "                rmse=(\"error\", lambda x: float(np.sqrt(np.mean(np.square(x))))),\n",
        "            )\n",
        "            .sort_values(\"mae\", ascending=False)\n",
        "            .reset_index()\n",
        "        )\n",
        "        group_df.to_csv(\n",
        "            OUTPUT_DIR / f\"metrics_by_{col.lower()}_{name.lower().replace(' ', '_')}.csv\",\n",
        "            index=False,\n",
        "        )\n",
        "\n",
        "        # Plot worst categories\n",
        "        fig = plot_error_by_category_custom(\n",
        "            subset,\n",
        "            category_col=col,\n",
        "            title=f\"MAE by {col} (Top 12 Worst) - {name} (2025)\",\n",
        "            top_n=12,\n",
        "        )\n",
        "        fig.update_xaxes(title=\"MAE (s)\")\n",
        "        fig.update_yaxes(title=col)\n",
        "        style_plot(fig, height=520)\n",
        "        save_plot(fig, OUTPUT_DIR / f\"mae_by_{col.lower()}_{name.lower().replace(' ', '_')}\")\n",
        "        fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Per-round metrics (2025) for each model\n",
        "round_frames = []\n",
        "for name in models.keys():\n",
        "    subset = errors_df[errors_df[\"model\"] == name]\n",
        "    if \"RoundNumber\" not in subset.columns:\n",
        "        continue\n",
        "    grouped = subset.groupby([\"RoundNumber\", \"EventName\"], dropna=False)\n",
        "    for (rnd, event), grp in grouped:\n",
        "        y_true_g = grp[\"y_true\"].to_numpy()\n",
        "        y_pred_g = grp[\"y_pred\"].to_numpy()\n",
        "        round_frames.append({\n",
        "            \"model\": name,\n",
        "            \"RoundNumber\": rnd,\n",
        "            \"EventName\": event,\n",
        "            \"n\": len(grp),\n",
        "            \"mae\": mean_absolute_error(y_true_g, y_pred_g),\n",
        "            \"rmse\": np.sqrt(mean_squared_error(y_true_g, y_pred_g)),\n",
        "            \"r2\": r2_score(y_true_g, y_pred_g) if len(y_true_g) > 1 else np.nan,\n",
        "        })\n",
        "\n",
        "round_df = pd.DataFrame(round_frames).sort_values([\"model\", \"RoundNumber\"])\n",
        "round_df.to_csv(OUTPUT_DIR / \"metrics_by_round_2025.csv\", index=False)\n",
        "round_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Plot MAE by round for each model\n",
        "if not round_df.empty:\n",
        "    fig = px.line(\n",
        "        round_df,\n",
        "        x=\"RoundNumber\",\n",
        "        y=\"mae\",\n",
        "        color=\"model\",\n",
        "        markers=True,\n",
        "        title=\"MAE by Round (2025)\",\n",
        "        labels={\"mae\": \"MAE (s)\", \"RoundNumber\": \"Round\"},\n",
        ")\n",
        "    style_plot(fig, height=450)\n",
        "    save_plot(fig, OUTPUT_DIR / \"mae_by_round_2025\")\n",
        "    fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}