{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 06 - Model Comparison (Full 2025 Test)\n",
    "\n",
    "**Purpose:** Comprehensive evaluation of all trained models on the complete 2025 season (unseen data).\n",
    "\n",
    "**MLOps Best Practices:**\n",
    "- Consistent model colors and styling across all plots\n",
    "- All metrics saved to CSV for reproducibility\n",
    "- Bootstrap confidence intervals for statistical rigor\n",
    "- Publication-ready plots (HTML + PNG)\n",
    "\n",
    "**Sections:**\n",
    "1. Configuration & Setup\n",
    "2. Data Loading (2025 Test Set)\n",
    "3. Model Loading & Inference\n",
    "4. **Comprehensive Numerical Metrics** (all metrics, no plots)\n",
    "5. Model Comparison Plots\n",
    "6. Per-Model Diagnostic Plots\n",
    "7. Error Analysis by Category\n",
    "8. Temporal Analysis (by Round)\n",
    "\n",
    "**Output:** `reports/notebooks/06_model_comparison_full/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parent if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from _common import load_dataset, prepare_features\n",
    "from src.split import SplitConfig\n",
    "from src.eval import compute_full_metrics\n",
    "from src.plots import save_plot\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = ROOT / \"reports\" / \"notebooks\" / \"06_model_comparison_full\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model paths\n",
    "MODELS_DIR = ROOT / \"reports\" / \"models\"\n",
    "MODEL_NAMES = [\"Linear\", \"XGBoost\", \"Deep MLP\"]\n",
    "\n",
    "# Consistent color scheme for all plots\n",
    "MODEL_COLORS = {\n",
    "    \"Linear\": \"#636EFA\",      # Blue\n",
    "    \"XGBoost\": \"#EF553B\",     # Red\n",
    "    \"Deep MLP\": \"#00CC96\",    # Green\n",
    "}\n",
    "\n",
    "# Plot styling\n",
    "PLOT_TEMPLATE = \"plotly_white\"\n",
    "PLOT_FONT_SIZE = 13\n",
    "PLOT_HEIGHT = 480\n",
    "PLOT_WIDTH = 800\n",
    "\n",
    "# Bootstrap settings\n",
    "BOOTSTRAP_N = 500\n",
    "BOOTSTRAP_CI = 95  # percent\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOTTING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def style_figure(fig, title=None, height=PLOT_HEIGHT, width=None):\n",
    "    \"\"\"Apply consistent styling to all figures.\"\"\"\n",
    "    fig.update_layout(\n",
    "        template=PLOT_TEMPLATE,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        title=dict(text=title or fig.layout.title.text, font=dict(size=16)),\n",
    "        font=dict(size=PLOT_FONT_SIZE),\n",
    "        legend=dict(font=dict(size=12)),\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        showline=True, linewidth=1, linecolor=\"black\", mirror=True,\n",
    "        ticks=\"outside\", showgrid=True, gridcolor=\"rgba(0,0,0,0.08)\",\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        showline=True, linewidth=1, linecolor=\"black\", mirror=True,\n",
    "        ticks=\"outside\", showgrid=True, gridcolor=\"rgba(0,0,0,0.08)\",\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_and_show(fig, name):\n",
    "    \"\"\"Save figure to HTML and PNG, then display.\"\"\"\n",
    "    save_plot(fig, OUTPUT_DIR / name)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading (2025 Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with full 2025 as test (test_rounds=None means all available)\n",
    "split_config = SplitConfig(test_rounds=None)\n",
    "df, metadata = load_dataset()\n",
    "train_df, val_df, trainval_df, test_df, features = prepare_features(\n",
    "    df, metadata, split_config=split_config\n",
    ")\n",
    "\n",
    "# Extract test arrays\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[\"LapTimeSeconds\"].to_numpy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET SUMMARY (2025 Season - Unseen Data)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total laps:     {len(test_df):,}\")\n",
    "print(f\"Features:       {len(features)}\")\n",
    "print(f\"Season(s):      {sorted(test_df['Season'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show test set composition\n",
    "test_summary = (\n",
    "    test_df.groupby([\"Season\", \"RoundNumber\", \"EventName\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Laps\")\n",
    "    .sort_values([\"Season\", \"RoundNumber\"])\n",
    ")\n",
    "print(f\"\\nRaces in test set: {len(test_summary)}\")\n",
    "print(f\"Rounds: {test_summary['RoundNumber'].tolist()}\")\n",
    "test_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Loading & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved models\n",
    "models = {}\n",
    "for name in MODEL_NAMES:\n",
    "    path = MODELS_DIR / f\"{name.lower().replace(' ', '_')}.joblib\"\n",
    "    if path.exists():\n",
    "        models[name] = joblib.load(path)\n",
    "        print(f\"Loaded: {name} from {path.name}\")\n",
    "    else:\n",
    "        print(f\"WARNING: {name} not found at {path}\")\n",
    "\n",
    "if not models:\n",
    "    raise FileNotFoundError(f\"No models found in {MODELS_DIR}. Run training notebooks first.\")\n",
    "\n",
    "print(f\"\\nModels loaded: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all models\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    print(f\"{name}: Generated {len(predictions[name]):,} predictions\")\n",
    "\n",
    "# Build unified error DataFrame for analysis\n",
    "meta_cols = [\n",
    "    \"Season\", \"RoundNumber\", \"EventName\", \"Circuit\",\n",
    "    \"Driver\", \"Team\", \"Compound\", \"TireAgeCategory\",\n",
    "    \"LapNumber\", \"Stint\", \"TyreLife\"\n",
    "]\n",
    "meta_cols = [c for c in meta_cols if c in test_df.columns]\n",
    "\n",
    "error_frames = []\n",
    "for name, y_pred in predictions.items():\n",
    "    frame = test_df[meta_cols].copy()\n",
    "    frame[\"model\"] = name\n",
    "    frame[\"y_true\"] = y_test\n",
    "    frame[\"y_pred\"] = y_pred\n",
    "    frame[\"error\"] = y_pred - y_test\n",
    "    frame[\"abs_error\"] = np.abs(frame[\"error\"])\n",
    "    frame[\"pct_error\"] = (frame[\"abs_error\"] / frame[\"y_true\"].clip(lower=1)) * 100\n",
    "    error_frames.append(frame)\n",
    "\n",
    "errors_df = pd.concat(error_frames, ignore_index=True)\n",
    "print(f\"\\nError analysis DataFrame: {len(errors_df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comprehensive Numerical Metrics\n",
    "\n",
    "**This cell outputs ALL numerical metrics for the paper - no plots, just numbers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE NUMERICAL METRICS (ALL MODELS)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_all_metrics(y_true, y_pred, n_features):\n",
    "    \"\"\"Compute comprehensive metrics for a single model.\"\"\"\n",
    "    n = len(y_true)\n",
    "    residuals = y_pred - y_true\n",
    "    abs_errors = np.abs(residuals)\n",
    "    \n",
    "    # Avoid division by zero for percentage metrics\n",
    "    y_true_safe = np.clip(np.abs(y_true), 1e-6, None)\n",
    "    \n",
    "    metrics = {\n",
    "        # Sample size\n",
    "        \"n_samples\": n,\n",
    "        \"n_features\": n_features,\n",
    "        \n",
    "        # === PRIMARY METRICS ===\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "        \n",
    "        # === ADJUSTED R2 ===\n",
    "        \"R2_adjusted\": 1 - (1 - r2_score(y_true, y_pred)) * (n - 1) / (n - n_features - 1),\n",
    "        \n",
    "        # === PERCENTAGE METRICS ===\n",
    "        \"MAPE_pct\": float(np.mean(abs_errors / y_true_safe) * 100),\n",
    "        \"sMAPE_pct\": float(np.mean(2 * abs_errors / (np.abs(y_true) + np.abs(y_pred) + 1e-6)) * 100),\n",
    "        \"RMSPE_pct\": float(np.sqrt(np.mean((abs_errors / y_true_safe) ** 2)) * 100),\n",
    "        \n",
    "        # === ERROR DISTRIBUTION ===\n",
    "        \"MedianAE\": float(np.median(abs_errors)),\n",
    "        \"MaxError\": float(np.max(abs_errors)),\n",
    "        \"MinError\": float(np.min(abs_errors)),\n",
    "        \"StdError\": float(np.std(residuals)),\n",
    "        \"MAD_error\": float(np.median(np.abs(residuals - np.median(residuals)))),\n",
    "        \n",
    "        # === BIAS METRICS ===\n",
    "        \"Bias_mean\": float(np.mean(residuals)),\n",
    "        \"Bias_median\": float(np.median(residuals)),\n",
    "        \n",
    "        # === PERCENTILES OF ABSOLUTE ERROR ===\n",
    "        \"P50_abs_error\": float(np.percentile(abs_errors, 50)),\n",
    "        \"P75_abs_error\": float(np.percentile(abs_errors, 75)),\n",
    "        \"P90_abs_error\": float(np.percentile(abs_errors, 90)),\n",
    "        \"P95_abs_error\": float(np.percentile(abs_errors, 95)),\n",
    "        \"P99_abs_error\": float(np.percentile(abs_errors, 99)),\n",
    "        \n",
    "        # === THRESHOLD ACCURACY ===\n",
    "        \"Within_0.5s_pct\": float((abs_errors <= 0.5).mean() * 100),\n",
    "        \"Within_1s_pct\": float((abs_errors <= 1.0).mean() * 100),\n",
    "        \"Within_2s_pct\": float((abs_errors <= 2.0).mean() * 100),\n",
    "        \"Within_3s_pct\": float((abs_errors <= 3.0).mean() * 100),\n",
    "        \"Within_5s_pct\": float((abs_errors <= 5.0).mean() * 100),\n",
    "        \n",
    "        # === CORRELATION ===\n",
    "        \"Pearson_r\": float(np.corrcoef(y_true, y_pred)[0, 1]),\n",
    "        \"Spearman_r\": float(pd.Series(y_true).corr(pd.Series(y_pred), method=\"spearman\")),\n",
    "        \n",
    "        # === DESCRIPTIVE STATS ===\n",
    "        \"Mean_actual\": float(np.mean(y_true)),\n",
    "        \"Std_actual\": float(np.std(y_true)),\n",
    "        \"Mean_predicted\": float(np.mean(y_pred)),\n",
    "        \"Std_predicted\": float(np.std(y_pred)),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Compute metrics for all models\n",
    "all_metrics = []\n",
    "for name, y_pred in predictions.items():\n",
    "    m = compute_all_metrics(y_test, y_pred, len(features))\n",
    "    m[\"Model\"] = name\n",
    "    all_metrics.append(m)\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Reorder columns\n",
    "col_order = [\"Model\", \"n_samples\", \"n_features\"] + [c for c in metrics_df.columns if c not in [\"Model\", \"n_samples\", \"n_features\"]]\n",
    "metrics_df = metrics_df[col_order].sort_values(\"MAE\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY: PRIMARY METRICS\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PRIMARY METRICS (2025 Test Set - Unseen Data)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "primary_cols = [\"Model\", \"n_samples\", \"MAE\", \"RMSE\", \"R2\", \"R2_adjusted\"]\n",
    "display(metrics_df[primary_cols].style.format({\n",
    "    \"MAE\": \"{:.4f}\",\n",
    "    \"RMSE\": \"{:.4f}\",\n",
    "    \"R2\": \"{:.4f}\",\n",
    "    \"R2_adjusted\": \"{:.4f}\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY: PERCENTAGE METRICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERCENTAGE METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pct_cols = [\"Model\", \"MAPE_pct\", \"sMAPE_pct\", \"RMSPE_pct\"]\n",
    "display(metrics_df[pct_cols].style.format({\n",
    "    \"MAPE_pct\": \"{:.2f}%\",\n",
    "    \"sMAPE_pct\": \"{:.2f}%\",\n",
    "    \"RMSPE_pct\": \"{:.2f}%\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY: ERROR DISTRIBUTION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERROR DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dist_cols = [\"Model\", \"MedianAE\", \"MAD_error\", \"StdError\", \"MinError\", \"MaxError\"]\n",
    "display(metrics_df[dist_cols].style.format({\n",
    "    \"MedianAE\": \"{:.4f}s\",\n",
    "    \"MAD_error\": \"{:.4f}s\",\n",
    "    \"StdError\": \"{:.4f}s\",\n",
    "    \"MinError\": \"{:.4f}s\",\n",
    "    \"MaxError\": \"{:.2f}s\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY: PERCENTILES OF ABSOLUTE ERROR\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ABSOLUTE ERROR PERCENTILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pctl_cols = [\"Model\", \"P50_abs_error\", \"P75_abs_error\", \"P90_abs_error\", \"P95_abs_error\", \"P99_abs_error\"]\n",
    "display(metrics_df[pctl_cols].style.format({\n",
    "    \"P50_abs_error\": \"{:.3f}s\",\n",
    "    \"P75_abs_error\": \"{:.3f}s\",\n",
    "    \"P90_abs_error\": \"{:.3f}s\",\n",
    "    \"P95_abs_error\": \"{:.3f}s\",\n",
    "    \"P99_abs_error\": \"{:.3f}s\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY: THRESHOLD ACCURACY (% within X seconds)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"THRESHOLD ACCURACY (% of predictions within X seconds)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "thresh_cols = [\"Model\", \"Within_0.5s_pct\", \"Within_1s_pct\", \"Within_2s_pct\", \"Within_3s_pct\", \"Within_5s_pct\"]\n",
    "display(metrics_df[thresh_cols].style.format({\n",
    "    \"Within_0.5s_pct\": \"{:.1f}%\",\n",
    "    \"Within_1s_pct\": \"{:.1f}%\",\n",
    "    \"Within_2s_pct\": \"{:.1f}%\",\n",
    "    \"Within_3s_pct\": \"{:.1f}%\",\n",
    "    \"Within_5s_pct\": \"{:.1f}%\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY: BIAS & CORRELATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BIAS & CORRELATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bias_cols = [\"Model\", \"Bias_mean\", \"Bias_median\", \"Pearson_r\", \"Spearman_r\"]\n",
    "display(metrics_df[bias_cols].style.format({\n",
    "    \"Bias_mean\": \"{:.4f}s\",\n",
    "    \"Bias_median\": \"{:.4f}s\",\n",
    "    \"Pearson_r\": \"{:.4f}\",\n",
    "    \"Spearman_r\": \"{:.4f}\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BOOTSTRAP CONFIDENCE INTERVALS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BOOTSTRAP CONFIDENCE INTERVALS ({BOOTSTRAP_CI}% CI, n={BOOTSTRAP_N})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def bootstrap_confidence_intervals(y_true, y_pred, n_boot=BOOTSTRAP_N, ci=BOOTSTRAP_CI):\n",
    "    \"\"\"Compute bootstrap confidence intervals for key metrics.\"\"\"\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    boot_mae, boot_rmse, boot_r2 = [], [], []\n",
    "    \n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        yt, yp = y_true[idx], y_pred[idx]\n",
    "        boot_mae.append(mean_absolute_error(yt, yp))\n",
    "        boot_rmse.append(np.sqrt(mean_squared_error(yt, yp)))\n",
    "        boot_r2.append(r2_score(yt, yp))\n",
    "    \n",
    "    alpha = (100 - ci) / 2\n",
    "    return {\n",
    "        \"MAE_CI\": f\"[{np.percentile(boot_mae, alpha):.4f}, {np.percentile(boot_mae, 100-alpha):.4f}]\",\n",
    "        \"RMSE_CI\": f\"[{np.percentile(boot_rmse, alpha):.4f}, {np.percentile(boot_rmse, 100-alpha):.4f}]\",\n",
    "        \"R2_CI\": f\"[{np.percentile(boot_r2, alpha):.4f}, {np.percentile(boot_r2, 100-alpha):.4f}]\",\n",
    "    }\n",
    "\n",
    "ci_results = []\n",
    "for name, y_pred in predictions.items():\n",
    "    ci = bootstrap_confidence_intervals(y_test, y_pred)\n",
    "    ci[\"Model\"] = name\n",
    "    ci_results.append(ci)\n",
    "\n",
    "ci_df = pd.DataFrame(ci_results)[[\"Model\", \"MAE_CI\", \"RMSE_CI\", \"R2_CI\"]]\n",
    "display(ci_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE ALL METRICS TO CSV\n",
    "# =============================================================================\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"metrics_comprehensive_2025.csv\", index=False)\n",
    "print(f\"\\nSaved: {OUTPUT_DIR / 'metrics_comprehensive_2025.csv'}\")\n",
    "\n",
    "# Save predictions for reproducibility\n",
    "errors_df.to_parquet(OUTPUT_DIR / \"predictions_2025.parquet\", index=False)\n",
    "print(f\"Saved: {OUTPUT_DIR / 'predictions_2025.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: MAE & RMSE Comparison\n",
    "# =============================================================================\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"Mean Absolute Error (MAE)\", \"Root Mean Square Error (RMSE)\"],\n",
    "    horizontal_spacing=0.12,\n",
    ")\n",
    "\n",
    "for i, metric in enumerate([\"MAE\", \"RMSE\"], 1):\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[row[\"Model\"]],\n",
    "                y=[row[metric]],\n",
    "                name=row[\"Model\"],\n",
    "                marker_color=MODEL_COLORS[row[\"Model\"]],\n",
    "                text=[f\"{row[metric]:.3f}s\"],\n",
    "                textposition=\"outside\",\n",
    "                showlegend=(i == 1),\n",
    "            ),\n",
    "            row=1, col=i,\n",
    "        )\n",
    "\n",
    "fig.update_yaxes(title_text=\"Seconds\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Seconds\", row=1, col=2)\n",
    "style_figure(fig, title=\"Model Comparison: Error Metrics (2025 Test Set)\", height=450, width=900)\n",
    "fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"center\", x=0.5))\n",
    "save_and_show(fig, \"model_comparison_mae_rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: R² Comparison\n",
    "# =============================================================================\n",
    "fig = go.Figure()\n",
    "\n",
    "for _, row in metrics_df.iterrows():\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[row[\"Model\"]],\n",
    "            y=[row[\"R2\"]],\n",
    "            name=row[\"Model\"],\n",
    "            marker_color=MODEL_COLORS[row[\"Model\"]],\n",
    "            text=[f\"{row['R2']:.4f}\"],\n",
    "            textposition=\"outside\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(title_text=\"R² Score\", range=[0, 1])\n",
    "fig.update_xaxes(title_text=\"Model\")\n",
    "style_figure(fig, title=\"Model Comparison: R² Score (2025 Test Set)\", height=420)\n",
    "save_and_show(fig, \"model_comparison_r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Threshold Accuracy (% within X seconds)\n",
    "# =============================================================================\n",
    "thresholds = [0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "thresh_data = []\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    abs_err = np.abs(y_test - y_pred)\n",
    "    for t in thresholds:\n",
    "        thresh_data.append({\n",
    "            \"Model\": name,\n",
    "            \"Threshold\": f\"<= {t}s\",\n",
    "            \"Percentage\": (abs_err <= t).mean() * 100,\n",
    "        })\n",
    "\n",
    "thresh_df = pd.DataFrame(thresh_data)\n",
    "\n",
    "fig = px.bar(\n",
    "    thresh_df,\n",
    "    x=\"Threshold\",\n",
    "    y=\"Percentage\",\n",
    "    color=\"Model\",\n",
    "    barmode=\"group\",\n",
    "    color_discrete_map=MODEL_COLORS,\n",
    "    text=thresh_df[\"Percentage\"].round(1).astype(str) + \"%\",\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition=\"outside\")\n",
    "fig.update_yaxes(title_text=\"% of Predictions\", range=[0, 105])\n",
    "fig.update_xaxes(title_text=\"Error Threshold\")\n",
    "style_figure(fig, title=\"Prediction Accuracy: % Within Error Thresholds (2025)\", height=480)\n",
    "save_and_show(fig, \"accuracy_thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Cumulative Error Distribution (CDF)\n",
    "# =============================================================================\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    abs_err = np.abs(y_test - y_pred)\n",
    "    sorted_err = np.sort(abs_err)\n",
    "    cumulative = np.arange(1, len(sorted_err) + 1) / len(sorted_err) * 100\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sorted_err,\n",
    "        y=cumulative,\n",
    "        mode=\"lines\",\n",
    "        name=name,\n",
    "        line=dict(color=MODEL_COLORS[name], width=2.5),\n",
    "    ))\n",
    "\n",
    "# Reference lines\n",
    "for t in [1, 2, 5]:\n",
    "    fig.add_vline(x=t, line_dash=\"dot\", line_color=\"gray\", opacity=0.5,\n",
    "                  annotation_text=f\"{t}s\", annotation_position=\"top\")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Absolute Error (seconds)\", range=[0, 10])\n",
    "fig.update_yaxes(title_text=\"Cumulative % of Predictions\")\n",
    "style_figure(fig, title=\"Cumulative Error Distribution (2025 Test Set)\", height=480)\n",
    "fig.update_layout(legend=dict(x=0.75, y=0.25))\n",
    "save_and_show(fig, \"cumulative_error_distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Per-Model Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Actual vs Predicted (per model)\n",
    "# =============================================================================\n",
    "for name, y_pred in predictions.items():\n",
    "    # Sample for visualization\n",
    "    n_sample = min(5000, len(y_test))\n",
    "    idx = np.random.choice(len(y_test), n_sample, replace=False)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_test[idx],\n",
    "        y=y_pred[idx],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=MODEL_COLORS[name], opacity=0.4, size=5),\n",
    "        name=\"Predictions\",\n",
    "    ))\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_v, max_v = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[min_v, max_v],\n",
    "        y=[min_v, max_v],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", dash=\"dash\", width=2),\n",
    "        name=\"Perfect Prediction\",\n",
    "    ))\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Actual Lap Time (s)\")\n",
    "    fig.update_yaxes(title_text=\"Predicted Lap Time (s)\")\n",
    "    style_figure(fig, title=f\"Actual vs Predicted: {name} (2025)\", height=500)\n",
    "    save_and_show(fig, f\"actual_vs_predicted_{name.lower().replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Residual Distribution (per model)\n",
    "# =============================================================================\n",
    "for name, y_pred in predictions.items():\n",
    "    residuals = y_pred - y_test\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=residuals,\n",
    "        nbinsx=80,\n",
    "        marker_color=MODEL_COLORS[name],\n",
    "        opacity=0.75,\n",
    "        name=\"Residuals\",\n",
    "    ))\n",
    "    \n",
    "    # Add vertical line at 0\n",
    "    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"red\", line_width=2)\n",
    "    \n",
    "    # Add mean annotation\n",
    "    mean_res = np.mean(residuals)\n",
    "    fig.add_annotation(\n",
    "        x=mean_res, y=0.95, yref=\"paper\",\n",
    "        text=f\"Mean: {mean_res:.3f}s\",\n",
    "        showarrow=True, arrowhead=2,\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Residual (Predicted - Actual) [s]\")\n",
    "    fig.update_yaxes(title_text=\"Count\")\n",
    "    style_figure(fig, title=f\"Residual Distribution: {name} (2025)\", height=420)\n",
    "    save_and_show(fig, f\"residuals_{name.lower().replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Residuals vs Predicted (per model) - Heteroscedasticity check\n",
    "# =============================================================================\n",
    "for name, y_pred in predictions.items():\n",
    "    residuals = y_pred - y_test\n",
    "    \n",
    "    # Sample for visualization\n",
    "    n_sample = min(5000, len(y_test))\n",
    "    idx = np.random.choice(len(y_test), n_sample, replace=False)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_pred[idx],\n",
    "        y=residuals[idx],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=MODEL_COLORS[name], opacity=0.4, size=5),\n",
    "        name=\"Residuals\",\n",
    "    ))\n",
    "    \n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", line_width=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Predicted Lap Time (s)\")\n",
    "    fig.update_yaxes(title_text=\"Residual (s)\")\n",
    "    style_figure(fig, title=f\"Residuals vs Predicted: {name} (2025)\", height=450)\n",
    "    save_and_show(fig, f\"residuals_vs_pred_{name.lower().replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ovfl27pkbj",
   "source": "---\n## 6b. XGBoost Feature Importance\n\nCritical for model interpretability - shows which features XGBoost relies on most.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eprrc3vd2rl",
   "source": "# =============================================================================\n# PLOT: XGBoost Feature Importance (Top 20)\n# =============================================================================\nif \"XGBoost\" in models:\n    xgb_model = models[\"XGBoost\"]\n    \n    # Extract XGBoost estimator from pipeline\n    if hasattr(xgb_model, \"named_steps\"):\n        xgb_estimator = xgb_model.named_steps.get(\"model\")\n    elif hasattr(xgb_model, \"steps\"):\n        xgb_estimator = xgb_model.steps[-1][1]\n    else:\n        xgb_estimator = xgb_model\n    \n    if hasattr(xgb_estimator, \"feature_importances_\"):\n        importances = xgb_estimator.feature_importances_\n        \n        # Map to feature names\n        if len(importances) == len(features):\n            feature_names = features\n        else:\n            feature_names = [f\"Feature_{i}\" for i in range(len(importances))]\n        \n        imp_df = pd.DataFrame({\n            \"Feature\": feature_names,\n            \"Importance\": importances\n        }).sort_values(\"Importance\", ascending=True).tail(20)\n        \n        # Save full importance to CSV\n        full_imp = pd.DataFrame({\n            \"Feature\": feature_names,\n            \"Importance\": importances\n        }).sort_values(\"Importance\", ascending=False)\n        full_imp.to_csv(OUTPUT_DIR / \"feature_importance_xgboost.csv\", index=False)\n        \n        fig = go.Figure()\n        fig.add_trace(go.Bar(\n            x=imp_df[\"Importance\"],\n            y=imp_df[\"Feature\"],\n            orientation=\"h\",\n            marker_color=MODEL_COLORS[\"XGBoost\"],\n            text=imp_df[\"Importance\"].round(4),\n            textposition=\"outside\",\n        ))\n        \n        fig.update_xaxes(title_text=\"Importance (Gain)\")\n        fig.update_yaxes(title_text=\"Feature\")\n        style_figure(fig, title=\"XGBoost Feature Importance (Top 20)\", height=600)\n        save_and_show(fig, \"feature_importance_xgboost\")\n    else:\n        print(\"XGBoost model has no feature_importances_ attribute.\")\nelse:\n    print(\"XGBoost model not loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Error Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ERROR BY CATEGORY: Helper Function\n",
    "# =============================================================================\n",
    "def plot_error_by_category(errors_df, category, model_name, top_n=12):\n",
    "    \"\"\"Create bar chart of MAE by category.\"\"\"\n",
    "    subset = errors_df[errors_df[\"model\"] == model_name]\n",
    "    \n",
    "    grouped = (\n",
    "        subset.groupby(category)\n",
    "        .agg(\n",
    "            n=(\"abs_error\", \"size\"),\n",
    "            mae=(\"abs_error\", \"mean\"),\n",
    "        )\n",
    "        .sort_values(\"mae\", ascending=False)\n",
    "        .head(top_n)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        grouped,\n",
    "        x=\"mae\",\n",
    "        y=category,\n",
    "        orientation=\"h\",\n",
    "        color_discrete_sequence=[MODEL_COLORS[model_name]],\n",
    "        text=grouped[\"mae\"].round(3).astype(str) + \"s\",\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(textposition=\"outside\")\n",
    "    fig.update_xaxes(title_text=\"MAE (seconds)\")\n",
    "    fig.update_yaxes(title_text=category, categoryorder=\"total ascending\")\n",
    "    \n",
    "    return fig, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Error by Circuit (best model)\n",
    "# =============================================================================\n",
    "best_model = metrics_df.iloc[0][\"Model\"]\n",
    "print(f\"Best model by MAE: {best_model}\")\n",
    "\n",
    "if \"Circuit\" in errors_df.columns:\n",
    "    fig, _ = plot_error_by_category(errors_df, \"Circuit\", best_model, top_n=15)\n",
    "    style_figure(fig, title=f\"MAE by Circuit: {best_model} (2025)\", height=520)\n",
    "    save_and_show(fig, f\"mae_by_circuit_{best_model.lower().replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Error by Compound (all models comparison)\n",
    "# =============================================================================\n",
    "if \"Compound\" in errors_df.columns:\n",
    "    compound_data = []\n",
    "    for name in models.keys():\n",
    "        subset = errors_df[errors_df[\"model\"] == name]\n",
    "        for compound in subset[\"Compound\"].unique():\n",
    "            mask = subset[\"Compound\"] == compound\n",
    "            if mask.sum() >= 10:\n",
    "                compound_data.append({\n",
    "                    \"Model\": name,\n",
    "                    \"Compound\": compound,\n",
    "                    \"MAE\": subset.loc[mask, \"abs_error\"].mean(),\n",
    "                    \"n\": mask.sum(),\n",
    "                })\n",
    "    \n",
    "    compound_df = pd.DataFrame(compound_data)\n",
    "    compound_df.to_csv(OUTPUT_DIR / \"mae_by_compound_all_models.csv\", index=False)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        compound_df,\n",
    "        x=\"Compound\",\n",
    "        y=\"MAE\",\n",
    "        color=\"Model\",\n",
    "        barmode=\"group\",\n",
    "        color_discrete_map=MODEL_COLORS,\n",
    "        text=compound_df[\"MAE\"].round(3).astype(str) + \"s\",\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(textposition=\"outside\")\n",
    "    fig.update_yaxes(title_text=\"MAE (seconds)\")\n",
    "    style_figure(fig, title=\"MAE by Tire Compound: All Models (2025)\", height=480)\n",
    "    save_and_show(fig, \"mae_by_compound_comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Error by Team (best model)\n",
    "# =============================================================================\n",
    "if \"Team\" in errors_df.columns:\n",
    "    fig, team_metrics = plot_error_by_category(errors_df, \"Team\", best_model, top_n=12)\n",
    "    style_figure(fig, title=f\"MAE by Team: {best_model} (2025)\", height=480)\n",
    "    team_metrics.to_csv(OUTPUT_DIR / f\"mae_by_team_{best_model.lower().replace(' ', '_')}.csv\", index=False)\n",
    "    save_and_show(fig, f\"mae_by_team_{best_model.lower().replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Error by Driver (best model)\n",
    "# =============================================================================\n",
    "if \"Driver\" in errors_df.columns:\n",
    "    fig, driver_metrics = plot_error_by_category(errors_df, \"Driver\", best_model, top_n=12)\n",
    "    style_figure(fig, title=f\"MAE by Driver: {best_model} (2025)\", height=520)\n",
    "    driver_metrics.to_csv(OUTPUT_DIR / f\"mae_by_driver_{best_model.lower().replace(' ', '_')}.csv\", index=False)\n",
    "    save_and_show(fig, f\"mae_by_driver_{best_model.lower().replace(' ', '_')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Temporal Analysis (by Round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# METRICS BY ROUND (all models)\n",
    "# =============================================================================\n",
    "round_metrics = []\n",
    "\n",
    "for name in models.keys():\n",
    "    subset = errors_df[errors_df[\"model\"] == name]\n",
    "    \n",
    "    for (rnd, event), grp in subset.groupby([\"RoundNumber\", \"EventName\"]):\n",
    "        y_true_g = grp[\"y_true\"].to_numpy()\n",
    "        y_pred_g = grp[\"y_pred\"].to_numpy()\n",
    "        \n",
    "        round_metrics.append({\n",
    "            \"Model\": name,\n",
    "            \"Round\": int(rnd),\n",
    "            \"Event\": event,\n",
    "            \"n\": len(grp),\n",
    "            \"MAE\": mean_absolute_error(y_true_g, y_pred_g),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_true_g, y_pred_g)),\n",
    "            \"R2\": r2_score(y_true_g, y_pred_g) if len(grp) > 1 else np.nan,\n",
    "        })\n",
    "\n",
    "round_df = pd.DataFrame(round_metrics).sort_values([\"Model\", \"Round\"])\n",
    "round_df.to_csv(OUTPUT_DIR / \"metrics_by_round_2025.csv\", index=False)\n",
    "\n",
    "display(round_df.pivot_table(index=[\"Round\", \"Event\"], columns=\"Model\", values=\"MAE\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: MAE by Round (all models)\n",
    "# =============================================================================\n",
    "fig = px.line(\n",
    "    round_df,\n",
    "    x=\"Round\",\n",
    "    y=\"MAE\",\n",
    "    color=\"Model\",\n",
    "    markers=True,\n",
    "    color_discrete_map=MODEL_COLORS,\n",
    "    hover_data=[\"Event\", \"n\"],\n",
    ")\n",
    "\n",
    "fig.update_traces(line_width=2.5, marker_size=8)\n",
    "fig.update_xaxes(title_text=\"Round Number\", dtick=1)\n",
    "fig.update_yaxes(title_text=\"MAE (seconds)\")\n",
    "style_figure(fig, title=\"MAE by Round: All Models (2025 Season)\", height=450)\n",
    "save_and_show(fig, \"mae_by_round_2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT: Error vs Lap Number (race progression)\n",
    "# =============================================================================\n",
    "if \"LapNumber\" in errors_df.columns:\n",
    "    subset = errors_df[errors_df[\"model\"] == best_model]\n",
    "    \n",
    "    lap_metrics = (\n",
    "        subset.groupby(\"LapNumber\")\n",
    "        .agg(\n",
    "            mae=(\"abs_error\", \"mean\"),\n",
    "            std=(\"abs_error\", \"std\"),\n",
    "            n=(\"abs_error\", \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    lap_metrics = lap_metrics[lap_metrics[\"n\"] >= 20]  # Filter low-sample laps\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Confidence band\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pd.concat([lap_metrics[\"LapNumber\"], lap_metrics[\"LapNumber\"][::-1]]),\n",
    "        y=pd.concat([lap_metrics[\"mae\"] + lap_metrics[\"std\"], \n",
    "                     (lap_metrics[\"mae\"] - lap_metrics[\"std\"])[::-1]]),\n",
    "        fill=\"toself\",\n",
    "        fillcolor=f\"rgba({int(MODEL_COLORS[best_model][1:3], 16)}, {int(MODEL_COLORS[best_model][3:5], 16)}, {int(MODEL_COLORS[best_model][5:7], 16)}, 0.2)\",\n",
    "        line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "        name=\"+/- 1 Std\",\n",
    "    ))\n",
    "    \n",
    "    # Mean line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=lap_metrics[\"LapNumber\"],\n",
    "        y=lap_metrics[\"mae\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"MAE\",\n",
    "        line=dict(color=MODEL_COLORS[best_model], width=2.5),\n",
    "        marker=dict(size=4),\n",
    "    ))\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Lap Number\")\n",
    "    fig.update_yaxes(title_text=\"MAE (seconds)\")\n",
    "    style_figure(fig, title=f\"Prediction Error vs Lap Number: {best_model} (2025)\", height=450)\n",
    "    save_and_show(fig, \"error_vs_lap_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "All metrics and plots have been saved to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIST ALL GENERATED OUTPUTS\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATED OUTPUTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDirectory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "csv_files = sorted(OUTPUT_DIR.glob(\"*.csv\"))\n",
    "png_files = sorted(OUTPUT_DIR.glob(\"*.png\"))\n",
    "html_files = sorted(OUTPUT_DIR.glob(\"*.html\"))\n",
    "parquet_files = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "print(f\"CSV files ({len(csv_files)}):\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nPlot files ({len(png_files)} PNG, {len(html_files)} HTML):\")\n",
    "for f in png_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nData files ({len(parquet_files)}):\")\n",
    "for f in parquet_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}